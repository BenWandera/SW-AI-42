"""
MobileViT-Small Architecture Explanation
A detailed breakdown of the model architecture used for waste classification
"""

import torch
import torch.nn as nn
import matplotlib.pyplot as plt
import matplotlib.patches as patches
from matplotlib.patches import FancyBboxPatch
import numpy as np

def explain_mobilevit_architecture():
    """
    Comprehensive explanation of MobileViT-Small architecture for waste classification
    """
    
    print("ğŸ—ï¸  MobileViT-Small Architecture for Waste Classification")
    print("=" * 70)
    
    print("""
ğŸ“± MobileViT (Mobile Vision Transformer) Overview:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

MobileViT combines the best of CNNs and Vision Transformers:
â€¢ ğŸ”„ CNNs for local spatial processing (early layers)
â€¢ ğŸ¯ Transformers for global context understanding (later layers)
â€¢ âš¡ Mobile-optimized design for efficiency
â€¢ ğŸ¨ Hierarchical feature extraction

Key Innovation: MobileViT blocks that seamlessly integrate convolutions 
with self-attention for both local and global feature learning.
""")

    # Architecture Components
    print("\nğŸ§± Architecture Components:")
    print("â”" * 40)
    
    components = [
        ("1ï¸âƒ£ Input Layer", "256Ã—256Ã—3 RGB images â†’ Preprocessed tensors"),
        ("2ï¸âƒ£ Stem Block", "Initial convolution + normalization"),
        ("3ï¸âƒ£ MV2 Blocks", "MobileNetV2-style inverted residual blocks"),
        ("4ï¸âƒ£ MobileViT Blocks", "Hybrid CNN-Transformer blocks"),
        ("5ï¸âƒ£ Global Pool", "Spatial feature aggregation"),
        ("6ï¸âƒ£ Custom Classifier", "Waste-specific classification head")
    ]
    
    for component, description in components:
        print(f"   {component:<20} {description}")
    
    print(f"\nğŸ“Š Model Statistics:")
    print(f"   â€¢ Total Parameters: 5,401,001 (~5.4M)")
    print(f"   â€¢ Trainable Parameters: 463,369 (~463K)")
    print(f"   â€¢ Model Size: ~21 MB")
    print(f"   â€¢ Input Resolution: 256Ã—256")
    print(f"   â€¢ Output Classes: 9 waste categories")
    
    return components

def detailed_architecture_breakdown():
    """
    Detailed layer-by-layer breakdown
    """
    
    print("\nğŸ” Detailed Architecture Breakdown:")
    print("â”" * 50)
    
    layers = [
        {
            "name": "Input Processing",
            "details": [
                "Input: 256Ã—256Ã—3 RGB images",
                "Preprocessing: Normalization (ImageNet stats)",
                "Data augmentation: Random flips, rotations, color jitter"
            ]
        },
        {
            "name": "Stem Block",
            "details": [
                "Conv2d: 3â†’16 channels, 3Ã—3 kernel, stride=2",
                "BatchNorm + SiLU activation",
                "Output: 128Ã—128Ã—16"
            ]
        },
        {
            "name": "MobileNetV2 Blocks (Stage 1)",
            "details": [
                "Block 1: 16â†’32 channels (depthwise separable)",
                "Block 2: 32â†’64 channels", 
                "Inverted residual structure with expansion",
                "Output: 32Ã—32Ã—64"
            ]
        },
        {
            "name": "MobileViT Block 1",
            "details": [
                "Input: 32Ã—32Ã—64",
                "Local processing: 3Ã—3 convolutions",
                "Global processing: Multi-head self-attention",
                "Patch size: 2Ã—2, Transformer dim: 144",
                "Output: 16Ã—16Ã—96"
            ]
        },
        {
            "name": "MobileViT Block 2", 
            "details": [
                "Input: 16Ã—16Ã—96",
                "Enhanced transformer processing",
                "Patch size: 2Ã—2, Transformer dim: 192",
                "Output: 8Ã—8Ã—128"
            ]
        },
        {
            "name": "MobileViT Block 3",
            "details": [
                "Input: 8Ã—8Ã—128",
                "Final feature extraction",
                "Patch size: 2Ã—2, Transformer dim: 240", 
                "Output: 4Ã—4Ã—160"
            ]
        },
        {
            "name": "Global Average Pooling",
            "details": [
                "Input: 4Ã—4Ã—160",
                "Spatial pooling: (4Ã—4) â†’ (1Ã—1)",
                "Output: 640-dimensional feature vector"
            ]
        },
        {
            "name": "Custom Classifier Head",
            "details": [
                "Linear 1: 640 â†’ 512 (+ BatchNorm + ReLU + Dropout)",
                "Linear 2: 512 â†’ 256 (+ BatchNorm + ReLU + Dropout)", 
                "Linear 3: 256 â†’ 9 (waste classes)",
                "Activation: Softmax for probabilities"
            ]
        }
    ]
    
    for i, layer in enumerate(layers, 1):
        print(f"\n{i}. {layer['name']}")
        print("   " + "â”€" * 30)
        for detail in layer['details']:
            print(f"   â€¢ {detail}")

def mobilevit_block_explanation():
    """
    Detailed explanation of the key MobileViT block
    """
    
    print("\nğŸ¯ MobileViT Block - The Core Innovation:")
    print("â”" * 50)
    
    print("""
The MobileViT block is where the magic happens! It combines:

ğŸ”„ Local Processing (CNN part):
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚  Input Feature Map                  â”‚
   â”‚         â†“                           â”‚
   â”‚  3Ã—3 Convolution (local features)   â”‚
   â”‚         â†“                           â”‚
   â”‚  1Ã—1 Convolution (channel mixing)   â”‚
   â”‚         â†“                           â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ğŸŒ Global Processing (Transformer part):  
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚  Unfold to patches (e.g., 2Ã—2)      â”‚
   â”‚         â†“                           â”‚
   â”‚  Multi-Head Self-Attention          â”‚
   â”‚         â†“                           â”‚
   â”‚  Feed-Forward Network               â”‚
   â”‚         â†“                           â”‚
   â”‚  Fold back to feature map           â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ğŸ”— Fusion:
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚  Concatenate local + global         â”‚
   â”‚         â†“                           â”‚
   â”‚  1Ã—1 Convolution (feature fusion)   â”‚
   â”‚         â†“                           â”‚
   â”‚  Output Feature Map                 â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
""")

def waste_classification_adaptation():
    """
    Explain how we adapted MobileViT for waste classification
    """
    
    print("\nğŸ—‚ï¸  Adaptation for Waste Classification:")
    print("â”" * 50)
    
    print("""
Original MobileViT â†’ Waste Classification MobileViT:

1ï¸âƒ£ Backbone Freezing:
   â€¢ Freeze pre-trained MobileViT weights (5M parameters)
   â€¢ Only train the classification head (463K parameters)
   â€¢ Reduces training time and prevents overfitting

2ï¸âƒ£ Custom Classification Head:
   Original: 640 â†’ 1000 (ImageNet classes)
   Our Model: 640 â†’ 512 â†’ 256 â†’ 9 (waste classes)
   
   Why 3 layers?
   â€¢ 640â†’512: Initial dimensionality reduction
   â€¢ 512â†’256: Further feature refinement  
   â€¢ 256â†’9: Final waste type classification

3ï¸âƒ£ Waste-Specific Optimizations:
   â€¢ Dropout (0.3, 0.4, 0.5): Prevent overfitting on waste data
   â€¢ BatchNorm: Stable training with different waste textures
   â€¢ ReLU: Non-linear feature learning
   â€¢ Label Smoothing: Handle similar waste categories

4ï¸âƒ£ Training Strategy:
   â€¢ Learning Rate: 0.001 (backbone) vs 0.0001 (classifier)
   â€¢ Data Augmentation: Rotations, flips, color changes
   â€¢ Early Stopping: Prevent overfitting
   â€¢ Cosine Annealing: Smooth learning rate decay
""")

def training_process_explanation():
    """
    Explain the training process
    """
    
    print("\nğŸš€ Training Process:")
    print("â”" * 30)
    
    print("""
Dataset Preparation (70/20/10 split):
â€¢ Training: 6,652 images â†’ Learn waste patterns
â€¢ Validation: 1,902 images â†’ Monitor performance  
â€¢ Testing: 950 images â†’ Final evaluation

Training Loop:
1. Load batch of waste images (8 images per batch)
2. Apply data augmentation (random transforms)
3. Forward pass through MobileViT:
   Input â†’ Stem â†’ MV2 Blocks â†’ MobileViT Blocks â†’ Pool â†’ Classifier
4. Calculate loss (CrossEntropy with label smoothing)
5. Backward pass (only update classifier weights)
6. Update weights with AdamW optimizer
7. Adjust learning rate with cosine scheduler

Monitoring:
â€¢ Track training/validation loss and accuracy
â€¢ Save best model based on validation accuracy
â€¢ Early stopping if no improvement for 8 epochs
â€¢ Generate confusion matrix and classification report
""")

def efficiency_benefits():
    """
    Explain efficiency benefits
    """
    
    print("\nâš¡ Efficiency Benefits:")
    print("â”" * 30)
    
    efficiency_points = [
        ("Model Size", "5.4M parameters vs 30M+ in larger models"),
        ("Training Speed", "Only 8.6% parameters trainable â†’ 10x faster"),
        ("Memory Usage", "~21MB model vs 100MB+ alternatives"),
        ("Inference Speed", "Mobile-optimized architecture"), 
        ("Data Efficiency", "Pre-trained backbone needs less waste data"),
        ("Accuracy", "Transformer attention captures global waste patterns")
    ]
    
    for metric, benefit in efficiency_points:
        print(f"   â€¢ {metric:<15}: {benefit}")

def create_architecture_diagram():
    """
    Create a visual diagram of the architecture
    """
    
    print("\nğŸ¨ Generating Architecture Diagram...")
    
    fig, ax = plt.subplots(1, 1, figsize=(14, 10))
    
    # Define components with positions and sizes
    components = [
        {"name": "Input\n256Ã—256Ã—3", "pos": (1, 8), "size": (1.5, 1), "color": "lightblue"},
        {"name": "Stem Block\n3Ã—3 Conv", "pos": (3, 8), "size": (1.5, 1), "color": "lightgreen"},
        {"name": "MV2 Block 1\n16â†’32", "pos": (5, 8), "size": (1.5, 1), "color": "lightcoral"},
        {"name": "MV2 Block 2\n32â†’64", "pos": (7, 8), "size": (1.5, 1), "color": "lightcoral"},
        {"name": "MobileViT 1\n64â†’96", "pos": (9, 8), "size": (1.5, 1), "color": "gold"},
        {"name": "MobileViT 2\n96â†’128", "pos": (11, 8), "size": (1.5, 1), "color": "gold"},
        {"name": "MobileViT 3\n128â†’160", "pos": (9, 6), "size": (1.5, 1), "color": "gold"},
        {"name": "Global Pool\n4Ã—4â†’1Ã—1", "pos": (7, 6), "size": (1.5, 1), "color": "plum"},
        {"name": "Linear 1\n640â†’512", "pos": (5, 6), "size": (1.5, 1), "color": "wheat"},
        {"name": "Linear 2\n512â†’256", "pos": (3, 6), "size": (1.5, 1), "color": "wheat"},
        {"name": "Output\n9 classes", "pos": (1, 6), "size": (1.5, 1), "color": "lightpink"},
    ]
    
    # Draw components
    for comp in components:
        rect = FancyBboxPatch(
            comp["pos"], comp["size"][0], comp["size"][1],
            boxstyle="round,pad=0.1",
            facecolor=comp["color"],
            edgecolor="black",
            linewidth=1.5
        )
        ax.add_patch(rect)
        
        # Add text
        ax.text(
            comp["pos"][0] + comp["size"][0]/2,
            comp["pos"][1] + comp["size"][1]/2,
            comp["name"],
            ha="center", va="center",
            fontsize=9, fontweight="bold"
        )
    
    # Draw arrows
    arrows = [
        ((2.5, 8.5), (3, 8.5)),    # Input â†’ Stem
        ((4.5, 8.5), (5, 8.5)),    # Stem â†’ MV2-1
        ((6.5, 8.5), (7, 8.5)),    # MV2-1 â†’ MV2-2
        ((8.5, 8.5), (9, 8.5)),    # MV2-2 â†’ MViT-1
        ((10.5, 8.5), (11, 8.5)),  # MViT-1 â†’ MViT-2
        ((11.75, 8), (10.25, 7)),  # MViT-2 â†’ MViT-3
        ((9.25, 7), (8.25, 6.5)),  # MViT-3 â†’ Pool
        ((7.25, 6.5), (6.25, 6.5)), # Pool â†’ Linear-1
        ((5.25, 6.5), (4.25, 6.5)), # Linear-1 â†’ Linear-2
        ((3.25, 6.5), (2.25, 6.5)), # Linear-2 â†’ Output
    ]
    
    for start, end in arrows:
        ax.annotate("", xy=end, xytext=start,
                   arrowprops=dict(arrowstyle="->", lw=2, color="darkblue"))
    
    # Add title and labels
    ax.set_title("MobileViT-Small Waste Classification Architecture", 
                fontsize=16, fontweight="bold", pad=20)
    
    # Add legend
    legend_elements = [
        patches.Patch(color='lightblue', label='Input'),
        patches.Patch(color='lightgreen', label='Stem'),
        patches.Patch(color='lightcoral', label='MobileNetV2 Blocks'),
        patches.Patch(color='gold', label='MobileViT Blocks'),
        patches.Patch(color='plum', label='Pooling'),
        patches.Patch(color='wheat', label='Classifier'),
        patches.Patch(color='lightpink', label='Output')
    ]
    ax.legend(handles=legend_elements, loc='upper left', bbox_to_anchor=(0.02, 0.98))
    
    # Set limits and remove axes
    ax.set_xlim(0, 13)
    ax.set_ylim(5, 10)
    ax.set_aspect('equal')
    ax.axis('off')
    
    plt.tight_layout()
    plt.savefig('mobilevit_architecture_diagram.png', dpi=300, bbox_inches='tight')
    plt.show()
    
    print("ğŸ“Š Architecture diagram saved as 'mobilevit_architecture_diagram.png'")

def main():
    """
    Run complete architecture explanation
    """
    
    # Basic overview
    explain_mobilevit_architecture()
    
    # Detailed breakdown
    detailed_architecture_breakdown()
    
    # Core innovation
    mobilevit_block_explanation()
    
    # Waste classification adaptation
    waste_classification_adaptation()
    
    # Training process
    training_process_explanation()
    
    # Efficiency benefits
    efficiency_benefits()
    
    # Visual diagram
    create_architecture_diagram()
    
    print(f"\nğŸ‰ Architecture Explanation Complete!")
    print(f"â”" * 50)
    print(f"""
Key Takeaways:
â€¢ MobileViT combines CNN efficiency with Transformer power
â€¢ Only 8.6% of parameters are trainable (efficient fine-tuning)
â€¢ Hierarchical feature extraction from local to global
â€¢ Optimized for mobile/edge deployment
â€¢ Perfect for waste classification with 9 categories
â€¢ Real-time inference capability with high accuracy
""")

if __name__ == "__main__":
    main()